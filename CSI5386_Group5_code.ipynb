{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI5386 Course Project - Automatic Poem Generation Using Deep Neural Networks\n",
    "# Group 5\n",
    "#### Abhilasha (300168332)\n",
    "#### Ravisha Sharma (300162406)\n",
    "#### Rajitha Muthukrishnan (300161725)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import contractions\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "import keras\n",
    "import keras.utils as ku\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from keras.layers import LSTM, Dense, Dropout, Flatten, Bidirectional, SimpleRNN\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import lm_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gutenberg Poetry Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = []\n",
    "for line in open(\"gutenberg-poetry-v001.ndjson\"):\n",
    "    all_lines.append(json.loads(line.strip()))\n",
    "    \n",
    "corpus = \"\\n\".join([line['s'] for line in random.sample(all_lines, 1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Of green-room, gambling-hell, saloon,\\nAnd loosed the props below.\\nAcross the land, by thee is shed:--\\nAttic maid, honey-fed, chatterer, snatchest thou and bearest the\\nSpontaneous beauties all around advance,\\nLet them take of my treasures, and clothes and steeds provide.\"\\nPale Grief, and pleasing Pain,\\nToo suddenly still and mute.\\nAnd it shows\\nBright was her body withal,     and golden cups her breasts.\\nThe sun will shine ageean.\\nThey blow an old-time way for me,\\nTo wedlock and the pastor\\'s daughter.\\nI know not how, I know not when,\\nWarm, hands, warm, daddy\\'s gone to plough;\\nDriven by an Onward-ache,\\nMy song shall raise the mountain-deer;\\nFrom my joys hath me removèd,\\nIn life or tenderest in heart.  I came\\natque alius latices pressis resupinus ab uuis\\nChe l\\'ale sue, tra liti si lontani\\nCaptain Pierce and his daughters\\n_Daffin_, merriment, foolishness.\\nIn his young charge\\'s throat: as if his crime\\nThen the blacksmith, Ilmarinen,\\nNor longer shall your princely flood\\nin the senseless stone'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand contracted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Of green-room, gambling-hell, saloon,\\nAnd loosed the props below.\\nAcross the land, by thee is she would:--\\nAttic maid, honey-fed, chatterer, snatchest thou and bearest the\\nSpontaneous beauties all around advance,\\nLet them take of my treasures, and clothes and steeds provide.\"\\nPale Grief, and pleasing Pain,\\nToo suddenly still and mute.\\nAnd it shows\\nBright was her body withal,     and golden cups her breasts.\\nThe sun will shine ageean.\\nThey blow an old-time way for me,\\nTo wedlock and the pastor\\'s daughter.\\nI know not how, I know not when,\\nWarm, hands, warm, daddy\\'s gone to plough;\\nDriven by an Onward-ache,\\nMy song shall raise the mountain-deer;\\nFrom my joys hath me removèd,\\nIn life or tenderest in heart.  I came\\natque alius latices pressis resupinus ab uuis\\nChe l\\'ale sue, tra liti si lontani\\nCaptain Pierce and his daughters\\n_Daffin_, merriment, foolishness.\\nIn his young charge\\'s throat: as if his crime\\nThen the blacksmith, Ilmarinen,\\nNor longer shall your princely flood\\nin the senseless '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_data = contractions.fix(corpus)\n",
    "expanded_data[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract new lines from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all text\n",
    "raw_text = expanded_data.lower()\n",
    "raw_text = raw_text\n",
    "raw_text = raw_text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unwanted punctuations from lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [txt.strip('\"!\\\"#$%&\\)*+-/(:;<=>?@][\\\\^_}{|~--[0-9]') for txt in raw_text]\n",
    "# text = raw_text.strip(\"\\\\'\\\"!\\\"#$%&\\)*+-/(:;<=>?@][\\^_}{|~--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of green-room, gambling-hell, saloon,',\n",
       " 'and loosed the props below.',\n",
       " 'across the land, by thee is she would',\n",
       " 'attic maid, honey-fed, chatterer, snatchest thou and bearest the',\n",
       " 'spontaneous beauties all around advance,',\n",
       " 'let them take of my treasures, and clothes and steeds provide.',\n",
       " 'pale grief, and pleasing pain,',\n",
       " 'too suddenly still and mute.',\n",
       " 'and it shows',\n",
       " 'bright was her body withal,     and golden cups her breasts.']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the lines extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tokenizer object to convert words to sequences of integers\n",
    "tokenizer = Tokenizer(num_words = None, filters = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = False)\n",
    "# tokenizer = Tokenizer(num_words = None, lower = False, char_level = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tokenizer to the texts\n",
    "tokenizer.fit_on_texts(text)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2544"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text to ngram sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of strings into flat dataset of sequences of tokens\n",
    "sequences = []\n",
    "for line in text:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6288"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad the sequences\n",
    "length to pad = max (length of lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to ensure equal lengths\n",
    "max_seq_len = max([len(x) for x in sequences])\n",
    "sequences = np.array(pad_sequences(sequences, maxlen = max_seq_len, padding = 'pre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-gram sequence - predictors and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create n-grams sequence predictors and labels\n",
    "predictors, label = sequences[:, :-1], sequences[:, -1]\n",
    "label = ku.to_categorical(label, num_classes = total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input length for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = max_seq_len - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words for poem generation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to monitor the generation of sentences in poem\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART - 1 : Markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_model = markovify.NewlineText(expanded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_poem_markov():\n",
    "    output_text = ''\n",
    "    for i in range(5):\n",
    "        for i in range(random.randrange(1, 4)):\n",
    "            text = markov_model.make_short_sentence(30)\n",
    "            print(text)\n",
    "            output_text += ' ' + text\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART - 2: Vanilla RNN - Simple word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 7.1759\n",
      "Epoch 2/100\n",
      "6288/6288 [==============================] - 3s 548us/step - loss: 6.6138\n",
      "Epoch 3/100\n",
      "6288/6288 [==============================] - 3s 548us/step - loss: 6.3079\n",
      "Epoch 4/100\n",
      "6288/6288 [==============================] - 3s 546us/step - loss: 5.9593\n",
      "Epoch 5/100\n",
      "6288/6288 [==============================] - 3s 541us/step - loss: 5.5873\n",
      "Epoch 6/100\n",
      "6288/6288 [==============================] - 3s 542us/step - loss: 5.2096\n",
      "Epoch 7/100\n",
      "6288/6288 [==============================] - 3s 541us/step - loss: 4.8209\n",
      "Epoch 8/100\n",
      "6288/6288 [==============================] - 3s 541us/step - loss: 4.4091\n",
      "Epoch 9/100\n",
      "6288/6288 [==============================] - 3s 542us/step - loss: 3.9994\n",
      "Epoch 10/100\n",
      "6288/6288 [==============================] - 3s 542us/step - loss: 3.6055\n",
      "Epoch 11/100\n",
      "6288/6288 [==============================] - 3s 542us/step - loss: 3.2219\n",
      "Epoch 12/100\n",
      "6288/6288 [==============================] - 3s 542us/step - loss: 2.8618\n",
      "Epoch 13/100\n",
      "6288/6288 [==============================] - 3s 542us/step - loss: 2.5421\n",
      "Epoch 14/100\n",
      "6288/6288 [==============================] - 3s 542us/step - loss: 2.2627\n",
      "Epoch 15/100\n",
      "6288/6288 [==============================] - 3s 542us/step - loss: 2.0111\n",
      "Epoch 16/100\n",
      "6288/6288 [==============================] - 3s 542us/step - loss: 1.7988\n",
      "Epoch 17/100\n",
      "6288/6288 [==============================] - 3s 542us/step - loss: 1.6142\n",
      "Epoch 18/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 1.4530\n",
      "Epoch 19/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 1.3154\n",
      "Epoch 20/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 1.1977\n",
      "Epoch 21/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 1.1008\n",
      "Epoch 22/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.9954\n",
      "Epoch 23/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.9232\n",
      "Epoch 24/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.8613\n",
      "Epoch 25/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.8066\n",
      "Epoch 26/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.7547\n",
      "Epoch 27/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.7089\n",
      "Epoch 28/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.6681\n",
      "Epoch 29/100\n",
      "6288/6288 [==============================] - 3s 542us/step - loss: 0.6471\n",
      "Epoch 30/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.6151\n",
      "Epoch 31/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.5919\n",
      "Epoch 32/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.5748\n",
      "Epoch 33/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.5592\n",
      "Epoch 34/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.5415\n",
      "Epoch 35/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.5273\n",
      "Epoch 36/100\n",
      "6288/6288 [==============================] - 3s 542us/step - loss: 0.5142\n",
      "Epoch 37/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.5123\n",
      "Epoch 38/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4991\n",
      "Epoch 39/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4957\n",
      "Epoch 40/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4814\n",
      "Epoch 41/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4780\n",
      "Epoch 42/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4776\n",
      "Epoch 43/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4743\n",
      "Epoch 44/100\n",
      "6288/6288 [==============================] - 3s 542us/step - loss: 0.4627\n",
      "Epoch 45/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4586\n",
      "Epoch 46/100\n",
      "6288/6288 [==============================] - 3s 542us/step - loss: 0.4592\n",
      "Epoch 47/100\n",
      "6288/6288 [==============================] - 3s 542us/step - loss: 0.4596\n",
      "Epoch 48/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4530\n",
      "Epoch 49/100\n",
      "6288/6288 [==============================] - 3s 542us/step - loss: 0.4582\n",
      "Epoch 50/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4485\n",
      "Epoch 51/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4485\n",
      "Epoch 52/100\n",
      "6288/6288 [==============================] - 3s 549us/step - loss: 0.4499\n",
      "Epoch 53/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4447\n",
      "Epoch 54/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4468\n",
      "Epoch 55/100\n",
      "6288/6288 [==============================] - 3s 549us/step - loss: 0.4460\n",
      "Epoch 56/100\n",
      "6288/6288 [==============================] - 3s 546us/step - loss: 0.4412\n",
      "Epoch 57/100\n",
      "6288/6288 [==============================] - 3s 548us/step - loss: 0.4363\n",
      "Epoch 58/100\n",
      "6288/6288 [==============================] - 3s 549us/step - loss: 0.4346\n",
      "Epoch 59/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4367\n",
      "Epoch 60/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4413\n",
      "Epoch 61/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4353\n",
      "Epoch 62/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4398\n",
      "Epoch 63/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4338\n",
      "Epoch 64/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4378\n",
      "Epoch 65/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4307\n",
      "Epoch 66/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4305\n",
      "Epoch 67/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4304\n",
      "Epoch 68/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4301\n",
      "Epoch 69/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4346\n",
      "Epoch 70/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4309\n",
      "Epoch 71/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4290\n",
      "Epoch 72/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4281\n",
      "Epoch 73/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4241\n",
      "Epoch 74/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4317\n",
      "Epoch 75/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4278\n",
      "Epoch 76/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4304\n",
      "Epoch 77/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4230\n",
      "Epoch 78/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4260\n",
      "Epoch 79/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4295\n",
      "Epoch 80/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4249\n",
      "Epoch 81/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4212\n",
      "Epoch 82/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4266\n",
      "Epoch 83/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4260\n",
      "Epoch 84/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4185\n",
      "Epoch 85/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4224\n",
      "Epoch 86/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4226\n",
      "Epoch 87/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4326\n",
      "Epoch 88/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4194\n",
      "Epoch 89/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4200\n",
      "Epoch 90/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4275\n",
      "Epoch 91/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4229\n",
      "Epoch 92/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4206\n",
      "Epoch 93/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4243\n",
      "Epoch 94/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4236\n",
      "Epoch 95/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4189\n",
      "Epoch 96/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4230\n",
      "Epoch 97/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4197\n",
      "Epoch 98/100\n",
      "6288/6288 [==============================] - 3s 544us/step - loss: 0.4153\n",
      "Epoch 99/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4234\n",
      "Epoch 100/100\n",
      "6288/6288 [==============================] - 3s 543us/step - loss: 0.4174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8b9048160>"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(total_words, 100, input_length = input_len))\n",
    "rnn_model.add(SimpleRNN(150))\n",
    "rnn_model.add(Dropout(0.1))\n",
    "rnn_model.add(Dense(total_words, activation = 'softmax'))\n",
    "rnn_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "# Use 100 epoch for efficacy\n",
    "rnn_model.fit(predictors, label, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART - 3: LSTM - Simple word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6288/6288 [==============================] - 4s 708us/step - loss: 7.1605\n",
      "Epoch 2/100\n",
      "6288/6288 [==============================] - 4s 666us/step - loss: 6.6384\n",
      "Epoch 3/100\n",
      "6288/6288 [==============================] - 4s 664us/step - loss: 6.4843\n",
      "Epoch 4/100\n",
      "6288/6288 [==============================] - 4s 664us/step - loss: 6.3412\n",
      "Epoch 5/100\n",
      "6288/6288 [==============================] - 4s 667us/step - loss: 6.1966\n",
      "Epoch 6/100\n",
      "6288/6288 [==============================] - 4s 672us/step - loss: 6.0451\n",
      "Epoch 7/100\n",
      "6288/6288 [==============================] - 4s 679us/step - loss: 5.8695\n",
      "Epoch 8/100\n",
      "6288/6288 [==============================] - 4s 669us/step - loss: 5.6741\n",
      "Epoch 9/100\n",
      "6288/6288 [==============================] - 4s 665us/step - loss: 5.4678\n",
      "Epoch 10/100\n",
      "6288/6288 [==============================] - 4s 662us/step - loss: 5.2622\n",
      "Epoch 11/100\n",
      "6288/6288 [==============================] - 4s 660us/step - loss: 5.0589\n",
      "Epoch 12/100\n",
      "6288/6288 [==============================] - 4s 663us/step - loss: 4.8536\n",
      "Epoch 13/100\n",
      "6288/6288 [==============================] - 4s 664us/step - loss: 4.6541\n",
      "Epoch 14/100\n",
      "6288/6288 [==============================] - 4s 664us/step - loss: 4.4496\n",
      "Epoch 15/100\n",
      "6288/6288 [==============================] - 4s 666us/step - loss: 4.2503\n",
      "Epoch 16/100\n",
      "6288/6288 [==============================] - 4s 666us/step - loss: 4.0534\n",
      "Epoch 17/100\n",
      "6288/6288 [==============================] - 4s 667us/step - loss: 3.8610\n",
      "Epoch 18/100\n",
      "6288/6288 [==============================] - 4s 669us/step - loss: 3.6694\n",
      "Epoch 19/100\n",
      "6288/6288 [==============================] - 4s 663us/step - loss: 3.4820\n",
      "Epoch 20/100\n",
      "6288/6288 [==============================] - 4s 662us/step - loss: 3.2903\n",
      "Epoch 21/100\n",
      "6288/6288 [==============================] - 4s 664us/step - loss: 3.1175\n",
      "Epoch 22/100\n",
      "6288/6288 [==============================] - 4s 664us/step - loss: 2.9450\n",
      "Epoch 23/100\n",
      "6288/6288 [==============================] - 4s 664us/step - loss: 2.7810\n",
      "Epoch 24/100\n",
      "6288/6288 [==============================] - 4s 665us/step - loss: 2.6223\n",
      "Epoch 25/100\n",
      "6288/6288 [==============================] - 4s 664us/step - loss: 2.4701\n",
      "Epoch 26/100\n",
      "6288/6288 [==============================] - 4s 664us/step - loss: 2.3387\n",
      "Epoch 27/100\n",
      "6288/6288 [==============================] - 4s 664us/step - loss: 2.1926\n",
      "Epoch 28/100\n",
      "6288/6288 [==============================] - 4s 665us/step - loss: 2.0716\n",
      "Epoch 29/100\n",
      "6288/6288 [==============================] - 4s 669us/step - loss: 1.9561\n",
      "Epoch 30/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 1.8414\n",
      "Epoch 31/100\n",
      "6288/6288 [==============================] - 4s 669us/step - loss: 1.7431\n",
      "Epoch 32/100\n",
      "6288/6288 [==============================] - 4s 673us/step - loss: 1.6465\n",
      "Epoch 33/100\n",
      "6288/6288 [==============================] - 4s 671us/step - loss: 1.5521\n",
      "Epoch 34/100\n",
      "6288/6288 [==============================] - 4s 665us/step - loss: 1.4658\n",
      "Epoch 35/100\n",
      "6288/6288 [==============================] - 4s 663us/step - loss: 1.3937\n",
      "Epoch 36/100\n",
      "6288/6288 [==============================] - 4s 664us/step - loss: 1.3243\n",
      "Epoch 37/100\n",
      "6288/6288 [==============================] - 4s 664us/step - loss: 1.2404\n",
      "Epoch 38/100\n",
      "6288/6288 [==============================] - 4s 665us/step - loss: 1.1790\n",
      "Epoch 39/100\n",
      "6288/6288 [==============================] - 4s 664us/step - loss: 1.1209\n",
      "Epoch 40/100\n",
      "6288/6288 [==============================] - 4s 665us/step - loss: 1.0707\n",
      "Epoch 41/100\n",
      "6288/6288 [==============================] - 4s 664us/step - loss: 1.0223\n",
      "Epoch 42/100\n",
      "6288/6288 [==============================] - 4s 663us/step - loss: 0.9695\n",
      "Epoch 43/100\n",
      "6288/6288 [==============================] - 4s 667us/step - loss: 0.9248\n",
      "Epoch 44/100\n",
      "6288/6288 [==============================] - 4s 673us/step - loss: 0.8883\n",
      "Epoch 45/100\n",
      "6288/6288 [==============================] - 4s 666us/step - loss: 0.8492\n",
      "Epoch 46/100\n",
      "6288/6288 [==============================] - 4s 666us/step - loss: 0.8082\n",
      "Epoch 47/100\n",
      "6288/6288 [==============================] - 4s 665us/step - loss: 0.7779\n",
      "Epoch 48/100\n",
      "6288/6288 [==============================] - 4s 666us/step - loss: 0.7389\n",
      "Epoch 49/100\n",
      "6288/6288 [==============================] - 4s 666us/step - loss: 0.7200\n",
      "Epoch 50/100\n",
      "6288/6288 [==============================] - 4s 667us/step - loss: 0.6996\n",
      "Epoch 51/100\n",
      "6288/6288 [==============================] - 4s 666us/step - loss: 0.6685\n",
      "Epoch 52/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.6463\n",
      "Epoch 53/100\n",
      "6288/6288 [==============================] - 4s 667us/step - loss: 0.6253\n",
      "Epoch 54/100\n",
      "6288/6288 [==============================] - 4s 666us/step - loss: 0.6063\n",
      "Epoch 55/100\n",
      "6288/6288 [==============================] - 4s 666us/step - loss: 0.5918\n",
      "Epoch 56/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.5753\n",
      "Epoch 57/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.5559\n",
      "Epoch 58/100\n",
      "6288/6288 [==============================] - 4s 666us/step - loss: 0.5491\n",
      "Epoch 59/100\n",
      "6288/6288 [==============================] - 4s 667us/step - loss: 0.5355\n",
      "Epoch 60/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.5170\n",
      "Epoch 61/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.5076\n",
      "Epoch 62/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.5004\n",
      "Epoch 63/100\n",
      "6288/6288 [==============================] - 4s 669us/step - loss: 0.4842\n",
      "Epoch 64/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.4759\n",
      "Epoch 65/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.4746\n",
      "Epoch 66/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.4684\n",
      "Epoch 67/100\n",
      "6288/6288 [==============================] - 4s 669us/step - loss: 0.4616\n",
      "Epoch 68/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.4567\n",
      "Epoch 69/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.4475\n",
      "Epoch 70/100\n",
      "6288/6288 [==============================] - 4s 669us/step - loss: 0.4418\n",
      "Epoch 71/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.4391\n",
      "Epoch 72/100\n",
      "6288/6288 [==============================] - 4s 669us/step - loss: 0.4301\n",
      "Epoch 73/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.4282\n",
      "Epoch 74/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.4203\n",
      "Epoch 75/100\n",
      "6288/6288 [==============================] - 4s 669us/step - loss: 0.4187\n",
      "Epoch 76/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.4126\n",
      "Epoch 77/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.4105\n",
      "Epoch 78/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.4106\n",
      "Epoch 79/100\n",
      "6288/6288 [==============================] - 4s 669us/step - loss: 0.4083\n",
      "Epoch 80/100\n",
      "6288/6288 [==============================] - 4s 669us/step - loss: 0.4090\n",
      "Epoch 81/100\n",
      "6288/6288 [==============================] - 4s 669us/step - loss: 0.4017\n",
      "Epoch 82/100\n",
      "6288/6288 [==============================] - 4s 668us/step - loss: 0.4059\n",
      "Epoch 83/100\n",
      "6288/6288 [==============================] - 4s 669us/step - loss: 0.4010\n",
      "Epoch 84/100\n",
      "6288/6288 [==============================] - 4s 669us/step - loss: 0.3949\n",
      "Epoch 85/100\n",
      "6288/6288 [==============================] - 4s 669us/step - loss: 0.3957\n",
      "Epoch 86/100\n",
      "6288/6288 [==============================] - 4s 670us/step - loss: 0.3933\n",
      "Epoch 87/100\n",
      "6288/6288 [==============================] - 4s 670us/step - loss: 0.3888\n",
      "Epoch 88/100\n",
      "6288/6288 [==============================] - 4s 673us/step - loss: 0.3895\n",
      "Epoch 89/100\n",
      "6288/6288 [==============================] - 4s 674us/step - loss: 0.3880\n",
      "Epoch 90/100\n",
      "6288/6288 [==============================] - 4s 677us/step - loss: 0.3840\n",
      "Epoch 91/100\n",
      "6288/6288 [==============================] - 4s 679us/step - loss: 0.3847\n",
      "Epoch 92/100\n",
      "6288/6288 [==============================] - 4s 678us/step - loss: 0.3812\n",
      "Epoch 93/100\n",
      "6288/6288 [==============================] - 4s 681us/step - loss: 0.3811\n",
      "Epoch 94/100\n",
      "6288/6288 [==============================] - 4s 680us/step - loss: 0.3808\n",
      "Epoch 95/100\n",
      "6288/6288 [==============================] - 4s 680us/step - loss: 0.3789\n",
      "Epoch 96/100\n",
      "6288/6288 [==============================] - 4s 681us/step - loss: 0.3786\n",
      "Epoch 97/100\n",
      "6288/6288 [==============================] - 4s 680us/step - loss: 0.3756\n",
      "Epoch 98/100\n",
      "6288/6288 [==============================] - 4s 681us/step - loss: 0.3763\n",
      "Epoch 99/100\n",
      "6288/6288 [==============================] - 4s 681us/step - loss: 0.3734\n",
      "Epoch 100/100\n",
      "6288/6288 [==============================] - 4s 678us/step - loss: 0.3807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8bc8c9760>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(total_words, 100, input_length = input_len))\n",
    "lstm_model.add(LSTM(150))\n",
    "lstm_model.add(Dropout(0.1))\n",
    "lstm_model.add(Dense(total_words, activation = 'softmax'))\n",
    "lstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "# Use 100 epoch for efficacy\n",
    "lstm_model.fit(predictors, label, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part - 4: Bidirectional LSTM - Simple word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 7.1970\n",
      "Epoch 2/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 6.6447\n",
      "Epoch 3/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 6.4629\n",
      "Epoch 4/100\n",
      "6288/6288 [==============================] - 10s 2ms/step - loss: 6.2784\n",
      "Epoch 5/100\n",
      "6288/6288 [==============================] - 7s 1ms/step - loss: 6.0727\n",
      "Epoch 6/100\n",
      "6288/6288 [==============================] - 7s 1ms/step - loss: 5.8668\n",
      "Epoch 7/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 5.6432\n",
      "Epoch 8/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 5.4399\n",
      "Epoch 9/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 5.2388\n",
      "Epoch 10/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 5.0204\n",
      "Epoch 11/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 4.8158\n",
      "Epoch 12/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 4.6164\n",
      "Epoch 13/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 4.4245\n",
      "Epoch 14/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 4.2234\n",
      "Epoch 15/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 4.0439\n",
      "Epoch 16/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 3.8427\n",
      "Epoch 17/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 3.6576\n",
      "Epoch 18/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 3.4730\n",
      "Epoch 19/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 3.2936\n",
      "Epoch 20/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 3.1084\n",
      "Epoch 21/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 2.9493\n",
      "Epoch 22/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 2.7804\n",
      "Epoch 23/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 2.6072\n",
      "Epoch 24/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 2.4679\n",
      "Epoch 25/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 2.3209\n",
      "Epoch 26/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 2.2038\n",
      "Epoch 27/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 2.0769\n",
      "Epoch 28/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 1.9623\n",
      "Epoch 29/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 1.8527\n",
      "Epoch 30/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 1.7511\n",
      "Epoch 31/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 1.6565\n",
      "Epoch 32/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 1.5602\n",
      "Epoch 33/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 1.4844\n",
      "Epoch 34/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 1.4033\n",
      "Epoch 35/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 1.3393\n",
      "Epoch 36/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 1.2859\n",
      "Epoch 37/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 1.2020\n",
      "Epoch 38/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 1.1424\n",
      "Epoch 39/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 1.0916\n",
      "Epoch 40/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 1.0507\n",
      "Epoch 41/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 1.0063\n",
      "Epoch 42/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.9627\n",
      "Epoch 43/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.9226\n",
      "Epoch 44/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.8747\n",
      "Epoch 45/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.8394\n",
      "Epoch 46/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.8210\n",
      "Epoch 47/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.7899\n",
      "Epoch 48/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.7563\n",
      "Epoch 49/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.7465\n",
      "Epoch 50/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.7111\n",
      "Epoch 51/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.6986\n",
      "Epoch 52/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.6792\n",
      "Epoch 53/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.6420\n",
      "Epoch 54/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.6343\n",
      "Epoch 55/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.6147\n",
      "Epoch 56/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.6142\n",
      "Epoch 57/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.5872\n",
      "Epoch 58/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.5703\n",
      "Epoch 59/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.5702\n",
      "Epoch 60/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.5806\n",
      "Epoch 61/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.5590\n",
      "Epoch 62/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.5422\n",
      "Epoch 63/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.5282\n",
      "Epoch 64/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.5178\n",
      "Epoch 65/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.5053\n",
      "Epoch 66/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.5065\n",
      "Epoch 67/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.5022\n",
      "Epoch 68/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4944\n",
      "Epoch 69/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4830\n",
      "Epoch 70/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4921\n",
      "Epoch 71/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4736\n",
      "Epoch 72/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4752\n",
      "Epoch 73/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4857\n",
      "Epoch 74/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4689\n",
      "Epoch 75/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4745\n",
      "Epoch 76/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4633\n",
      "Epoch 77/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4647\n",
      "Epoch 78/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4503\n",
      "Epoch 79/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4428\n",
      "Epoch 80/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4386\n",
      "Epoch 81/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4343\n",
      "Epoch 82/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4261\n",
      "Epoch 83/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4293\n",
      "Epoch 84/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4297\n",
      "Epoch 85/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4416\n",
      "Epoch 86/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4388\n",
      "Epoch 87/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4288\n",
      "Epoch 88/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4347\n",
      "Epoch 89/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4327\n",
      "Epoch 90/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4264\n",
      "Epoch 91/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4246\n",
      "Epoch 92/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4130\n",
      "Epoch 93/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4201\n",
      "Epoch 94/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4198\n",
      "Epoch 95/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4240\n",
      "Epoch 96/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4204\n",
      "Epoch 97/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4131\n",
      "Epoch 98/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4060\n",
      "Epoch 99/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4043\n",
      "Epoch 100/100\n",
      "6288/6288 [==============================] - 8s 1ms/step - loss: 0.4012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8bc6fba00>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lstm_model = Sequential()\n",
    "bi_lstm_model.add(Embedding(total_words, 100, input_length = input_len))\n",
    "bi_lstm_model.add(Bidirectional(LSTM(100)))\n",
    "bi_lstm_model.add(Dropout(0.1))\n",
    "bi_lstm_model.add(Dense(total_words, activation = 'softmax'))\n",
    "bi_lstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "# Use 100 epoch for efficacy\n",
    "bi_lstm_model.fit(predictors, label, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2543\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_embedding_matrix(word_index):\n",
    "    embedded_words = {}\n",
    "    with open('glove.6B.100d.txt') as file:\n",
    "        for line in file:\n",
    "            words, coeff = line.split(maxsplit=1)\n",
    "            coeff = np.array(coeff.split(),dtype = float)\n",
    "            embedded_words[words] = coeff\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, 100))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedded_words.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = cal_embedding_matrix(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (2544, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of embedding matrix:',embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part - 5 : Vanilla RNN - Glove embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6288/6288 [==============================] - 7s 1ms/step - loss: 7.1870\n",
      "Epoch 2/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 6.4723\n",
      "Epoch 3/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 6.0119\n",
      "Epoch 4/100\n",
      "6288/6288 [==============================] - 3s 554us/step - loss: 5.5098\n",
      "Epoch 5/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 4.9860\n",
      "Epoch 6/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 4.4376\n",
      "Epoch 7/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 3.9021\n",
      "Epoch 8/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 3.3845\n",
      "Epoch 9/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 2.8973\n",
      "Epoch 10/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 2.4723\n",
      "Epoch 11/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 2.1036\n",
      "Epoch 12/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 1.8123\n",
      "Epoch 13/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 1.5574\n",
      "Epoch 14/100\n",
      "6288/6288 [==============================] - 3s 554us/step - loss: 1.3625\n",
      "Epoch 15/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 1.1991\n",
      "Epoch 16/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 1.0641\n",
      "Epoch 17/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 0.9573\n",
      "Epoch 18/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.8678\n",
      "Epoch 19/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.7946\n",
      "Epoch 20/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 0.7368\n",
      "Epoch 21/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.6871\n",
      "Epoch 22/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 0.6490\n",
      "Epoch 23/100\n",
      "6288/6288 [==============================] - 3s 554us/step - loss: 0.6227\n",
      "Epoch 24/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 0.5913\n",
      "Epoch 25/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 0.5624\n",
      "Epoch 26/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 0.5505\n",
      "Epoch 27/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 0.5354\n",
      "Epoch 28/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 0.5215\n",
      "Epoch 29/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 0.5103\n",
      "Epoch 30/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4952\n",
      "Epoch 31/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 0.4944\n",
      "Epoch 32/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 0.4815\n",
      "Epoch 33/100\n",
      "6288/6288 [==============================] - 3s 555us/step - loss: 0.4788\n",
      "Epoch 34/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 0.4729\n",
      "Epoch 35/100\n",
      "6288/6288 [==============================] - 3s 554us/step - loss: 0.4691\n",
      "Epoch 36/100\n",
      "6288/6288 [==============================] - 3s 554us/step - loss: 0.4632\n",
      "Epoch 37/100\n",
      "6288/6288 [==============================] - 3s 550us/step - loss: 0.4593\n",
      "Epoch 38/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4539\n",
      "Epoch 39/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4573\n",
      "Epoch 40/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 0.4537\n",
      "Epoch 41/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4511\n",
      "Epoch 42/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 0.4495\n",
      "Epoch 43/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 0.4450\n",
      "Epoch 44/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 0.4528\n",
      "Epoch 45/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4489\n",
      "Epoch 46/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4432\n",
      "Epoch 47/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4451\n",
      "Epoch 48/100\n",
      "6288/6288 [==============================] - 3s 550us/step - loss: 0.4352\n",
      "Epoch 49/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 0.4358\n",
      "Epoch 50/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4337\n",
      "Epoch 51/100\n",
      "6288/6288 [==============================] - 3s 550us/step - loss: 0.4385\n",
      "Epoch 52/100\n",
      "6288/6288 [==============================] - 3s 550us/step - loss: 0.4409\n",
      "Epoch 53/100\n",
      "6288/6288 [==============================] - 3s 550us/step - loss: 0.4381\n",
      "Epoch 54/100\n",
      "6288/6288 [==============================] - 3s 550us/step - loss: 0.4308\n",
      "Epoch 55/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 0.4406\n",
      "Epoch 56/100\n",
      "6288/6288 [==============================] - 3s 549us/step - loss: 0.4388\n",
      "Epoch 57/100\n",
      "6288/6288 [==============================] - 3s 550us/step - loss: 0.4402\n",
      "Epoch 58/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 0.4351\n",
      "Epoch 59/100\n",
      "6288/6288 [==============================] - 3s 548us/step - loss: 0.4246\n",
      "Epoch 60/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4314\n",
      "Epoch 61/100\n",
      "6288/6288 [==============================] - 3s 549us/step - loss: 0.4298\n",
      "Epoch 62/100\n",
      "6288/6288 [==============================] - 3s 550us/step - loss: 0.4322\n",
      "Epoch 63/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4264\n",
      "Epoch 64/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4255\n",
      "Epoch 65/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 0.4273\n",
      "Epoch 66/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 0.4266\n",
      "Epoch 67/100\n",
      "6288/6288 [==============================] - 3s 554us/step - loss: 0.4215\n",
      "Epoch 68/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 0.4300\n",
      "Epoch 69/100\n",
      "6288/6288 [==============================] - 3s 550us/step - loss: 0.4254\n",
      "Epoch 70/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4220\n",
      "Epoch 71/100\n",
      "6288/6288 [==============================] - 3s 549us/step - loss: 0.4223\n",
      "Epoch 72/100\n",
      "6288/6288 [==============================] - 3s 550us/step - loss: 0.4235\n",
      "Epoch 73/100\n",
      "6288/6288 [==============================] - 3s 549us/step - loss: 0.4278\n",
      "Epoch 74/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4232\n",
      "Epoch 75/100\n",
      "6288/6288 [==============================] - 3s 552us/step - loss: 0.4271\n",
      "Epoch 76/100\n",
      "6288/6288 [==============================] - 3s 549us/step - loss: 0.4221\n",
      "Epoch 77/100\n",
      "6288/6288 [==============================] - 3s 549us/step - loss: 0.4293\n",
      "Epoch 78/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4276\n",
      "Epoch 79/100\n",
      "6288/6288 [==============================] - 3s 550us/step - loss: 0.4218\n",
      "Epoch 80/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4264\n",
      "Epoch 81/100\n",
      "6288/6288 [==============================] - 3s 550us/step - loss: 0.4201\n",
      "Epoch 82/100\n",
      "6288/6288 [==============================] - 3s 553us/step - loss: 0.4265\n",
      "Epoch 83/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4217\n",
      "Epoch 84/100\n",
      "6288/6288 [==============================] - 3s 549us/step - loss: 0.4209\n",
      "Epoch 85/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4224\n",
      "Epoch 86/100\n",
      "6288/6288 [==============================] - 3s 548us/step - loss: 0.4239\n",
      "Epoch 87/100\n",
      "6288/6288 [==============================] - 3s 550us/step - loss: 0.4201\n",
      "Epoch 88/100\n",
      "6288/6288 [==============================] - 3s 549us/step - loss: 0.4152\n",
      "Epoch 89/100\n",
      "6288/6288 [==============================] - 3s 549us/step - loss: 0.4226\n",
      "Epoch 90/100\n",
      "6288/6288 [==============================] - 3s 550us/step - loss: 0.4205\n",
      "Epoch 91/100\n",
      "6288/6288 [==============================] - 3s 548us/step - loss: 0.4245\n",
      "Epoch 92/100\n",
      "6288/6288 [==============================] - 3s 548us/step - loss: 0.4230\n",
      "Epoch 93/100\n",
      "6288/6288 [==============================] - 3s 548us/step - loss: 0.4251\n",
      "Epoch 94/100\n",
      "6288/6288 [==============================] - 3s 548us/step - loss: 0.4236\n",
      "Epoch 95/100\n",
      "6288/6288 [==============================] - 3s 549us/step - loss: 0.4211\n",
      "Epoch 96/100\n",
      "6288/6288 [==============================] - 3s 547us/step - loss: 0.4158\n",
      "Epoch 97/100\n",
      "6288/6288 [==============================] - 3s 549us/step - loss: 0.4191\n",
      "Epoch 98/100\n",
      "6288/6288 [==============================] - 3s 549us/step - loss: 0.4156\n",
      "Epoch 99/100\n",
      "6288/6288 [==============================] - 3s 551us/step - loss: 0.4086\n",
      "Epoch 100/100\n",
      "6288/6288 [==============================] - 3s 548us/step - loss: 0.4301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faabf9c30d0>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_glove_model = Sequential()\n",
    "rnn_glove_model.add(Embedding(len(word_index)+1, 100, weights=[embedding_matrix], input_length = input_len))\n",
    "rnn_glove_model.add(SimpleRNN(150))\n",
    "rnn_glove_model.add(Dropout(0.1))\n",
    "rnn_glove_model.add(Dense(total_words, activation = 'softmax'))\n",
    "rnn_glove_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "# Use 100 epoch for efficacy\n",
    "rnn_glove_model.fit(predictors, label, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART - 6 : LSTM - Glove embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6288/6288 [==============================] - 11s 2ms/step - loss: 7.1635\n",
      "Epoch 2/100\n",
      "6288/6288 [==============================] - 4s 677us/step - loss: 6.5974\n",
      "Epoch 3/100\n",
      "6288/6288 [==============================] - 4s 692us/step - loss: 6.3443\n",
      "Epoch 4/100\n",
      "6288/6288 [==============================] - 4s 703us/step - loss: 6.0577\n",
      "Epoch 5/100\n",
      "6288/6288 [==============================] - 5s 720us/step - loss: 5.7538\n",
      "Epoch 6/100\n",
      "6288/6288 [==============================] - 5s 734us/step - loss: 5.4339\n",
      "Epoch 7/100\n",
      "6288/6288 [==============================] - 5s 746us/step - loss: 5.1003\n",
      "Epoch 8/100\n",
      "6288/6288 [==============================] - 5s 754us/step - loss: 4.7590\n",
      "Epoch 9/100\n",
      "6288/6288 [==============================] - 5s 778us/step - loss: 4.4083\n",
      "Epoch 10/100\n",
      "6288/6288 [==============================] - 5s 793us/step - loss: 4.0554\n",
      "Epoch 11/100\n",
      "6288/6288 [==============================] - 5s 823us/step - loss: 3.7087\n",
      "Epoch 12/100\n",
      "6288/6288 [==============================] - 5s 834us/step - loss: 3.3791\n",
      "Epoch 13/100\n",
      "6288/6288 [==============================] - 5s 871us/step - loss: 3.0598\n",
      "Epoch 14/100\n",
      "6288/6288 [==============================] - 6s 895us/step - loss: 2.7560\n",
      "Epoch 15/100\n",
      "6288/6288 [==============================] - 6s 945us/step - loss: 2.4869\n",
      "Epoch 16/100\n",
      "6288/6288 [==============================] - 6s 955us/step - loss: 2.2445\n",
      "Epoch 17/100\n",
      "6288/6288 [==============================] - 6s 969us/step - loss: 2.0195\n",
      "Epoch 18/100\n",
      "6288/6288 [==============================] - 6s 891us/step - loss: 1.8300\n",
      "Epoch 19/100\n",
      "6288/6288 [==============================] - 6s 909us/step - loss: 1.6648\n",
      "Epoch 20/100\n",
      "6288/6288 [==============================] - 6s 911us/step - loss: 1.5231\n",
      "Epoch 21/100\n",
      "6288/6288 [==============================] - 6s 930us/step - loss: 1.3823\n",
      "Epoch 22/100\n",
      "6288/6288 [==============================] - 6s 906us/step - loss: 1.2723\n",
      "Epoch 23/100\n",
      "6288/6288 [==============================] - 6s 908us/step - loss: 1.1685\n",
      "Epoch 24/100\n",
      "6288/6288 [==============================] - 6s 930us/step - loss: 1.0794\n",
      "Epoch 25/100\n",
      "6288/6288 [==============================] - 6s 905us/step - loss: 0.9986\n",
      "Epoch 26/100\n",
      "6288/6288 [==============================] - 6s 895us/step - loss: 0.9255\n",
      "Epoch 27/100\n",
      "6288/6288 [==============================] - 6s 883us/step - loss: 0.8663\n",
      "Epoch 28/100\n",
      "6288/6288 [==============================] - 6s 881us/step - loss: 0.8124\n",
      "Epoch 29/100\n",
      "6288/6288 [==============================] - 5s 865us/step - loss: 0.7601\n",
      "Epoch 30/100\n",
      "6288/6288 [==============================] - 6s 909us/step - loss: 0.7223\n",
      "Epoch 31/100\n",
      "6288/6288 [==============================] - 6s 939us/step - loss: 0.6835\n",
      "Epoch 32/100\n",
      "6288/6288 [==============================] - 6s 929us/step - loss: 0.6518\n",
      "Epoch 33/100\n",
      "6288/6288 [==============================] - 6s 1ms/step - loss: 0.6226\n",
      "Epoch 34/100\n",
      "6288/6288 [==============================] - 6s 953us/step - loss: 0.5981\n",
      "Epoch 35/100\n",
      "6288/6288 [==============================] - 6s 905us/step - loss: 0.5678\n",
      "Epoch 36/100\n",
      "6288/6288 [==============================] - 6s 881us/step - loss: 0.5540\n",
      "Epoch 37/100\n",
      "6288/6288 [==============================] - 6s 880us/step - loss: 0.5324\n",
      "Epoch 38/100\n",
      "6288/6288 [==============================] - 5s 855us/step - loss: 0.5219\n",
      "Epoch 39/100\n",
      "6288/6288 [==============================] - 6s 884us/step - loss: 0.5045\n",
      "Epoch 40/100\n",
      "6288/6288 [==============================] - 5s 856us/step - loss: 0.4980\n",
      "Epoch 41/100\n",
      "6288/6288 [==============================] - 5s 855us/step - loss: 0.4808\n",
      "Epoch 42/100\n",
      "6288/6288 [==============================] - 5s 853us/step - loss: 0.4784\n",
      "Epoch 43/100\n",
      "6288/6288 [==============================] - 5s 851us/step - loss: 0.4662\n",
      "Epoch 44/100\n",
      "6288/6288 [==============================] - 6s 876us/step - loss: 0.4659\n",
      "Epoch 45/100\n",
      "6288/6288 [==============================] - 5s 864us/step - loss: 0.4506\n",
      "Epoch 46/100\n",
      "6288/6288 [==============================] - 6s 886us/step - loss: 0.4520\n",
      "Epoch 47/100\n",
      "6288/6288 [==============================] - 6s 907us/step - loss: 0.4453\n",
      "Epoch 48/100\n",
      "6288/6288 [==============================] - 6s 891us/step - loss: 0.4348\n",
      "Epoch 49/100\n",
      "6288/6288 [==============================] - 6s 918us/step - loss: 0.4363\n",
      "Epoch 50/100\n",
      "6288/6288 [==============================] - 6s 892us/step - loss: 0.4256\n",
      "Epoch 51/100\n",
      "6288/6288 [==============================] - 6s 905us/step - loss: 0.4242\n",
      "Epoch 52/100\n",
      "6288/6288 [==============================] - 6s 928us/step - loss: 0.4193\n",
      "Epoch 53/100\n",
      "6288/6288 [==============================] - 6s 903us/step - loss: 0.4173\n",
      "Epoch 54/100\n",
      "6288/6288 [==============================] - 6s 887us/step - loss: 0.4184\n",
      "Epoch 55/100\n",
      "6288/6288 [==============================] - 6s 928us/step - loss: 0.4128\n",
      "Epoch 56/100\n",
      "6288/6288 [==============================] - 6s 951us/step - loss: 0.4155\n",
      "Epoch 57/100\n",
      "6288/6288 [==============================] - 6s 954us/step - loss: 0.4090\n",
      "Epoch 58/100\n",
      "6288/6288 [==============================] - 6s 901us/step - loss: 0.4040\n",
      "Epoch 59/100\n",
      "6288/6288 [==============================] - 6s 895us/step - loss: 0.4089\n",
      "Epoch 60/100\n",
      "6288/6288 [==============================] - 6s 896us/step - loss: 0.4020\n",
      "Epoch 61/100\n",
      "6288/6288 [==============================] - 6s 896us/step - loss: 0.4048\n",
      "Epoch 62/100\n",
      "6288/6288 [==============================] - 6s 898us/step - loss: 0.3948\n",
      "Epoch 63/100\n",
      "6288/6288 [==============================] - 6s 890us/step - loss: 0.3976\n",
      "Epoch 64/100\n",
      "6288/6288 [==============================] - 5s 855us/step - loss: 0.3954\n",
      "Epoch 65/100\n",
      "6288/6288 [==============================] - 5s 855us/step - loss: 0.3883\n",
      "Epoch 66/100\n",
      "6288/6288 [==============================] - 5s 857us/step - loss: 0.3885\n",
      "Epoch 67/100\n",
      "6288/6288 [==============================] - 5s 855us/step - loss: 0.3909\n",
      "Epoch 68/100\n",
      "6288/6288 [==============================] - 5s 857us/step - loss: 0.3939\n",
      "Epoch 69/100\n",
      "6288/6288 [==============================] - 5s 857us/step - loss: 0.3886\n",
      "Epoch 70/100\n",
      "6288/6288 [==============================] - 5s 860us/step - loss: 0.3879\n",
      "Epoch 71/100\n",
      "6288/6288 [==============================] - 5s 856us/step - loss: 0.3823\n",
      "Epoch 72/100\n",
      "6288/6288 [==============================] - 5s 857us/step - loss: 0.3874\n",
      "Epoch 73/100\n",
      "6288/6288 [==============================] - 5s 858us/step - loss: 0.3814\n",
      "Epoch 74/100\n",
      "6288/6288 [==============================] - 5s 864us/step - loss: 0.3819\n",
      "Epoch 75/100\n",
      "6288/6288 [==============================] - 5s 861us/step - loss: 0.3803\n",
      "Epoch 76/100\n",
      "6288/6288 [==============================] - 6s 888us/step - loss: 0.3837\n",
      "Epoch 77/100\n",
      "6288/6288 [==============================] - 6s 927us/step - loss: 0.3816\n",
      "Epoch 78/100\n",
      "6288/6288 [==============================] - 6s 935us/step - loss: 0.3790\n",
      "Epoch 79/100\n",
      "6288/6288 [==============================] - 6s 890us/step - loss: 0.3798\n",
      "Epoch 80/100\n",
      "6288/6288 [==============================] - 5s 858us/step - loss: 0.3770\n",
      "Epoch 81/100\n",
      "6288/6288 [==============================] - 5s 859us/step - loss: 0.3768\n",
      "Epoch 82/100\n",
      "6288/6288 [==============================] - 5s 861us/step - loss: 0.3753\n",
      "Epoch 83/100\n",
      "6288/6288 [==============================] - 5s 859us/step - loss: 0.3792\n",
      "Epoch 84/100\n",
      "6288/6288 [==============================] - 6s 884us/step - loss: 0.3784\n",
      "Epoch 85/100\n",
      "6288/6288 [==============================] - 6s 948us/step - loss: 0.3737\n",
      "Epoch 86/100\n",
      "6288/6288 [==============================] - 6s 901us/step - loss: 0.3681\n",
      "Epoch 87/100\n",
      "6288/6288 [==============================] - 6s 902us/step - loss: 0.3733\n",
      "Epoch 88/100\n",
      "6288/6288 [==============================] - 6s 899us/step - loss: 0.3747\n",
      "Epoch 89/100\n",
      "6288/6288 [==============================] - 6s 933us/step - loss: 0.3727\n",
      "Epoch 90/100\n",
      "6288/6288 [==============================] - 6s 946us/step - loss: 0.3759\n",
      "Epoch 91/100\n",
      "6288/6288 [==============================] - 6s 922us/step - loss: 0.3726\n",
      "Epoch 92/100\n",
      "6288/6288 [==============================] - 6s 928us/step - loss: 0.3676\n",
      "Epoch 93/100\n",
      "6288/6288 [==============================] - 6s 944us/step - loss: 0.3673\n",
      "Epoch 94/100\n",
      "6288/6288 [==============================] - 6s 971us/step - loss: 0.3704\n",
      "Epoch 95/100\n",
      "6288/6288 [==============================] - 6s 993us/step - loss: 0.3686\n",
      "Epoch 96/100\n",
      "6288/6288 [==============================] - 6s 959us/step - loss: 0.3678\n",
      "Epoch 97/100\n",
      "6288/6288 [==============================] - 6s 974us/step - loss: 0.3713\n",
      "Epoch 98/100\n",
      "6288/6288 [==============================] - 6s 1ms/step - loss: 0.3642\n",
      "Epoch 99/100\n",
      "6288/6288 [==============================] - 6s 982us/step - loss: 0.3645\n",
      "Epoch 100/100\n",
      "6288/6288 [==============================] - 6s 985us/step - loss: 0.3651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8d05e2460>"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_glove_model = Sequential()\n",
    "lstm_glove_model.add(Embedding(len(word_index)+1, 100, weights=[embedding_matrix], input_length = input_len))\n",
    "lstm_glove_model.add(LSTM(150))\n",
    "lstm_glove_model.add(Dropout(0.1))\n",
    "lstm_glove_model.add(Dense(total_words, activation = 'softmax'))\n",
    "lstm_glove_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "# Use 100 epoch for efficacy\n",
    "lstm_glove_model.fit(predictors, label, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part - 7: Bi-directional LSTM Glove embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 2629 of 2991 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6288/6288 [==============================] - 26s 4ms/step - loss: 7.1492\n",
      "Epoch 2/100\n",
      "6288/6288 [==============================] - 13s 2ms/step - loss: 6.5816\n",
      "Epoch 3/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 6.2788\n",
      "Epoch 4/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 5.9490\n",
      "Epoch 5/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 5.6035\n",
      "Epoch 6/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 5.2513\n",
      "Epoch 7/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 4.8908\n",
      "Epoch 8/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 4.5193\n",
      "Epoch 9/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 4.1393\n",
      "Epoch 10/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 3.7807\n",
      "Epoch 11/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 3.4181\n",
      "Epoch 12/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 3.0686\n",
      "Epoch 13/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 2.7450\n",
      "Epoch 14/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 2.4574\n",
      "Epoch 15/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 2.1892\n",
      "Epoch 16/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 1.9713\n",
      "Epoch 17/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 1.7597\n",
      "Epoch 18/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 1.5838\n",
      "Epoch 19/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 1.4310\n",
      "Epoch 20/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 1.3070\n",
      "Epoch 21/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 1.1965\n",
      "Epoch 22/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 1.0862\n",
      "Epoch 23/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 1.0030\n",
      "Epoch 24/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.9392\n",
      "Epoch 25/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.8701\n",
      "Epoch 26/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.8110\n",
      "Epoch 27/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.7588\n",
      "Epoch 28/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.7246\n",
      "Epoch 29/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.6785\n",
      "Epoch 30/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.6525\n",
      "Epoch 31/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.6221\n",
      "Epoch 32/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.5977\n",
      "Epoch 33/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.5725\n",
      "Epoch 34/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.5586\n",
      "Epoch 35/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.5377\n",
      "Epoch 36/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.5245\n",
      "Epoch 37/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.5079\n",
      "Epoch 38/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4986\n",
      "Epoch 39/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4854\n",
      "Epoch 40/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4794\n",
      "Epoch 41/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4686\n",
      "Epoch 42/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4617\n",
      "Epoch 43/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4624\n",
      "Epoch 44/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4507\n",
      "Epoch 45/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4432\n",
      "Epoch 46/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4405\n",
      "Epoch 47/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4383\n",
      "Epoch 48/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4365\n",
      "Epoch 49/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4262\n",
      "Epoch 50/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4283\n",
      "Epoch 51/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4282\n",
      "Epoch 52/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4274\n",
      "Epoch 53/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4210\n",
      "Epoch 54/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4157\n",
      "Epoch 55/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4135\n",
      "Epoch 56/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4097\n",
      "Epoch 57/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4109\n",
      "Epoch 58/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4086\n",
      "Epoch 59/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4074\n",
      "Epoch 60/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4127\n",
      "Epoch 61/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.4015\n",
      "Epoch 62/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3971\n",
      "Epoch 63/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3949\n",
      "Epoch 64/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3972\n",
      "Epoch 65/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3980\n",
      "Epoch 66/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3915\n",
      "Epoch 67/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3914\n",
      "Epoch 68/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3907\n",
      "Epoch 69/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3869\n",
      "Epoch 70/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3974\n",
      "Epoch 71/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3865\n",
      "Epoch 72/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3903\n",
      "Epoch 73/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3911\n",
      "Epoch 74/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3801\n",
      "Epoch 75/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3838\n",
      "Epoch 76/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3866\n",
      "Epoch 77/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3829\n",
      "Epoch 78/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3860\n",
      "Epoch 79/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3819\n",
      "Epoch 80/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3824\n",
      "Epoch 81/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3817\n",
      "Epoch 82/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3775\n",
      "Epoch 83/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3776\n",
      "Epoch 84/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3866\n",
      "Epoch 85/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3768\n",
      "Epoch 86/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3787\n",
      "Epoch 87/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3759\n",
      "Epoch 88/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3691\n",
      "Epoch 89/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3728\n",
      "Epoch 90/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3776\n",
      "Epoch 91/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3730\n",
      "Epoch 92/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3766\n",
      "Epoch 93/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3755\n",
      "Epoch 94/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3803\n",
      "Epoch 95/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3734\n",
      "Epoch 96/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3745\n",
      "Epoch 97/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3730\n",
      "Epoch 98/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3722\n",
      "Epoch 99/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3731\n",
      "Epoch 100/100\n",
      "6288/6288 [==============================] - 12s 2ms/step - loss: 0.3673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8b8cce9d0>"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lstm_glove_model = Sequential()\n",
    "bi_lstm_glove_model.add(Embedding(len(word_index)+1, 100, weights=[embedding_matrix], input_length = input_len))\n",
    "bi_lstm_glove_model.add(Bidirectional(LSTM(100)))\n",
    "bi_lstm_glove_model.add(Dropout(0.1))\n",
    "bi_lstm_glove_model.add(Dense(total_words, activation = 'softmax'))\n",
    "bi_lstm_glove_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "# Use 100 epoch for efficacy\n",
    "bi_lstm_glove_model.fit(predictors, label, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POEM generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate line - based on STOP WORDS at the end of generated sentence. And every sentence should have only 5 words\n",
    "def generate_line(init_text, max_seq_len, num_sen, model):\n",
    "    output_word = ''\n",
    "    num_words = 5  \n",
    "    last_word = 'a'\n",
    "    text = init_text\n",
    "    init_words = nltk.word_tokenize(text)\n",
    "    \n",
    "    while last_word in stop_words:\n",
    "        if num_sen == 0:\n",
    "            num_words = num_words - len(init_words)\n",
    "    \n",
    "        output_text = ''\n",
    "        for j in range(num_words):\n",
    "            token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "            token_list = pad_sequences([token_list], maxlen = max_seq_len - 1, padding = 'pre')\n",
    "            predicted = model.predict_classes(token_list, verbose = 0)\n",
    "\n",
    "            for word, index in tokenizer.word_index.items():\n",
    "                if index == predicted:\n",
    "                    output_word = word\n",
    "                    break\n",
    "\n",
    "            text += ' ' + output_word\n",
    "            if num_sen == 0:\n",
    "                output_text = text\n",
    "            else:       \n",
    "                output_text += ' ' + output_word\n",
    "        words = nltk.word_tokenize(output_text)\n",
    "        last_word = words[-1]\n",
    "        \n",
    "    return output_text  \n",
    "\n",
    "def format_line(text, num_sen, sen):\n",
    "    text = text[:1].upper() + text[1:]\n",
    "    words = nltk.word_tokenize(text)\n",
    "    last_word = words[-1]\n",
    "    if sen != num_sen-1:\n",
    "        if last_word not in stop_words:\n",
    "            text = text + ','\n",
    "    else:\n",
    "        text = text + '.'\n",
    "    return text\n",
    "\n",
    "# Function to generate poem - multiple lines\n",
    "## Arguments (sample text, padding length, number of sentences needed, model)\n",
    "def generate_poem(text, max_seq_len, num_sen, model):\n",
    "    output_sentence = text\n",
    "    output = ''\n",
    "    for sen in range(num_sen):\n",
    "        gen_sent = generate_line(output_sentence, max_seq_len, sen, model)  \n",
    "        sentence = format_line(gen_sent, num_sen, sen)\n",
    "        output_sentence += '' + sentence\n",
    "        print(sentence)\n",
    "        output += sentence\n",
    "        text = output_sentence\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poem generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Went ruffling up the past,\n",
      "To wedlock and the high,\n",
      "And of the awful maid;\n",
      "And chaplets on the land;\n",
      "Who, like the starry lights;\n",
      "And that is free.\n",
      "Then to the faithful,\n",
      "Who will not be one of them.\n",
      "And of the flower.\n",
      "And as the moon spun,\n",
      "Here, all around advance,\n"
     ]
    }
   ],
   "source": [
    "markov_text = gen_poem_markov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN - Simple word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full of hope and passion,\n",
      " victory are barest voices sweet,\n",
      " that almost seem to drop,\n",
      " man seems night the past,\n",
      " all the king come back.\n"
     ]
    }
   ],
   "source": [
    "## Arguments - (sample text, padding length, number of sentences needed, model)\n",
    "# gen_text_rnn = generate_poem(\"Love to\", max_seq_len, 5, rnn_model)\n",
    "gen_text_rnn = generate_poem(\"Full of hope\", max_seq_len, 5, rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Full of hope and passion, victory are barest voices sweet, that almost seem to drop, man seems night the past, all the king come back.'"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - Simple word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full of hope and passion,\n",
      " more i more are boast,\n",
      " brought kept boast sails longs,\n",
      " to him to the rescue,\n",
      " has are open more would.\n"
     ]
    }
   ],
   "source": [
    "## Arguments - (sample text, padding length, number of sentences needed, model)\n",
    "# gen_text_lstm = generate_poem(\"Love to\", max_seq_len, 5, lstm_model)\n",
    "gen_text_lstm = generate_poem(\"Full of hope\", max_seq_len, 5, lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Full of hope and passion, more i more are boast, brought kept boast sails longs, to him to the rescue, has are open more would.'"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM - Simple word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full of hope and passion,\n",
      " hundred moss grown altars steep,\n",
      " steep disguises of actors qu,\n",
      " il il went chuck darkness,\n",
      " reached to one voices voices.\n"
     ]
    }
   ],
   "source": [
    "## Arguments - (sample text, padding length, number of sentences needed, model)\n",
    "# gen_text_bilstm = generate_poem(\"Love to\", max_seq_len, 5, bi_lstm_model)\n",
    "gen_text_bilstm = generate_poem(\"Full of hope\", max_seq_len, 5, bi_lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Full of hope and passion, hundred moss grown altars steep, steep disguises of actors qu, il il went chuck darkness, reached to one voices voices.'"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text_bilstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN - Glove embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full of hope and passion,\n",
      " or leave it their eyes,\n",
      " you orter seen the pettibone,\n",
      " shef doover unaim'd are love,\n",
      " in heaven the lordly meed.\n"
     ]
    }
   ],
   "source": [
    "## Arguments - (sample text, padding length, number of sentences needed, model)\n",
    "# gen_text_rnn_glove = generate_poem(\"Love to\", max_seq_len, 5, rnn_glove_model)\n",
    "gen_text_rnn_glove = generate_poem(\"Full of hope\", max_seq_len, 5, rnn_glove_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Full of hope and passion, or leave it their eyes, you orter seen the pettibone, shef doover unaim'd are love, in heaven the lordly meed.\""
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text_rnn_glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - Glove embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full of hope and passion,\n",
      " free more thrall of new,\n",
      " some some some at memory,\n",
      " them fell on some barbarian,\n",
      " see as it were bound.\n"
     ]
    }
   ],
   "source": [
    "## Arguments - (sample text, padding length, number of sentences needed, model)\n",
    "gen_text_lstm_glove = generate_poem(\"Full of hope\", max_seq_len, 5, lstm_glove_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Full of hope and passion, free more thrall of new, some some some at memory, them fell on some barbarian, see as it were bound.'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text_lstm_glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi Directional LSTM - Glove embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full of hope and passion,\n",
      " mind and say dryden's less,\n",
      " to me are good spread,\n",
      " me to say their eyes,\n",
      " awake to say their offends.\n"
     ]
    }
   ],
   "source": [
    "## Arguments - (sample text, padding length, number of sentences needed, model)\n",
    "# Sweet life, There he sang\n",
    "gen_text_bilstm_glove = generate_poem(\"Full of hope\", max_seq_len, 5, bi_lstm_glove_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Full of hope and passion, mind and say dryden's less, to me are good spread, me to say their eyes, awake to say their offends.\""
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text_bilstm_glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate - Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_scorer.models.auto import AutoLMScorer\n",
    "scorer = AutoLMScorer.from_pretrained(\"gpt2-large\")\n",
    "\n",
    "def prob_score(sentence):\n",
    "    return scorer.sentence_score(sentence, reduce='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markov model text\n",
      " Went ruffling up the past, To wedlock and the high, And of the awful maid; And chaplets on the land; Who, like the starry lights; And that is free. Then to the faithful, Who will not be one of them. And of the flower. And as the moon spun, Here, all around advance,\n",
      "0.08049044013023376\n"
     ]
    }
   ],
   "source": [
    "print('Markov model text')\n",
    "print(markov_text)\n",
    "print(prob_score(markov_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla RNN - tokenized data Model text\n",
      "Full of hope and passion, victory are barest voices sweet, that almost seem to drop, man seems night the past, all the king come back.\n",
      "0.06673873960971832\n"
     ]
    }
   ],
   "source": [
    "print('Vanilla RNN - tokenized data Model text')\n",
    "print(gen_text_rnn)\n",
    "print(prob_score(gen_text_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM - tokenized data Model text\n",
      "Full of hope and passion, more i more are boast, brought kept boast sails longs, to him to the rescue, has are open more would.\n",
      "0.05296219885349274\n"
     ]
    }
   ],
   "source": [
    "print('LSTM - tokenized data Model text')\n",
    "print(gen_text_lstm)\n",
    "print(prob_score(gen_text_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM - tokenized data Model text\n",
      "Full of hope and passion, hundred moss grown altars steep, steep disguises of actors qu, il il went chuck darkness, reached to one voices voices.\n",
      "0.06630679965019226\n"
     ]
    }
   ],
   "source": [
    "print('BiLSTM - tokenized data Model text')\n",
    "print(gen_text_bilstm)\n",
    "print(prob_score(gen_text_bilstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla RNN - glove embedding Model text\n",
      "Full of hope and passion, or leave it their eyes, you orter seen the pettibone, shef doover unaim'd are love, in heaven the lordly meed.\n",
      "0.05898677930235863\n"
     ]
    }
   ],
   "source": [
    "print('Vanilla RNN - glove embedding Model text')\n",
    "print(gen_text_rnn_glove)\n",
    "print(prob_score(gen_text_rnn_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM - glove embedding Model text\n",
      "Full of hope and passion, free more thrall of new, some some some at memory, them fell on some barbarian, see as it were bound.\n",
      "0.07978662103414536\n"
     ]
    }
   ],
   "source": [
    "print('LSTM - glove embedding Model text')\n",
    "print(gen_text_lstm_glove)\n",
    "print(prob_score(gen_text_lstm_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidirectional LSTM - glove embedding Model text\n",
      "Full of hope and passion, mind and say dryden's less, to me are good spread, me to say their eyes, awake to say their offends.\n",
      "0.05788024887442589\n"
     ]
    }
   ],
   "source": [
    "print('Bidirectional LSTM - glove embedding Model text')\n",
    "print(gen_text_bilstm_glove)\n",
    "print(prob_score(gen_text_bilstm_glove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate - Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel, OpenAIGPTLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 478750579/478750579 [20:24<00:00, 391116.84B/s] \n",
      "100%|██████████| 656/656 [00:00<00:00, 233689.78B/s]\n",
      "100%|██████████| 815973/815973 [00:00<00:00, 1000199.55B/s]\n",
      "100%|██████████| 458495/458495 [00:00<00:00, 977841.16B/s]\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "gpt_model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
    "gpt_model.eval()\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "gpt_tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppl_score(sentence):\n",
    "    tokenize_input = gpt_tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([gpt_tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    loss = gpt_model(tensor_input, lm_labels=tensor_input)\n",
    "    return math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markov model text\n",
      " Went ruffling up the past, To wedlock and the high, And of the awful maid; And chaplets on the land; Who, like the starry lights; And that is free. Then to the faithful, Who will not be one of them. And of the flower. And as the moon spun, Here, all around advance,\n",
      "167.33391422642183\n"
     ]
    }
   ],
   "source": [
    "print('Markov model text')\n",
    "print(markov_text)\n",
    "print(ppl_score(markov_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla RNN - tokenized data Model text\n",
      "Full of hope and passion, victory are barest voices sweet, that almost seem to drop, man seems night the past, all the king come back.\n",
      "486.6997205576134\n"
     ]
    }
   ],
   "source": [
    "print('Vanilla RNN - tokenized data Model text')\n",
    "print(gen_text_rnn)\n",
    "print(ppl_score(gen_text_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM - tokenized data Model text\n",
      "Full of hope and passion, more i more are boast, brought kept boast sails longs, to him to the rescue, has are open more would.\n",
      "1007.7326672166716\n"
     ]
    }
   ],
   "source": [
    "print('LSTM - tokenized data Model text')\n",
    "print(gen_text_lstm)\n",
    "print(ppl_score(gen_text_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM - tokenized data Model text\n",
      "Full of hope and passion, hundred moss grown altars steep, steep disguises of actors qu, il il went chuck darkness, reached to one voices voices.\n",
      "1194.1185692984177\n"
     ]
    }
   ],
   "source": [
    "print('BiLSTM - tokenized data Model text')\n",
    "print(gen_text_bilstm)\n",
    "print(ppl_score(gen_text_bilstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla RNN - glove embedding Model text\n",
      "Full of hope and passion, or leave it their eyes, you orter seen the pettibone, shef doover unaim'd are love, in heaven the lordly meed.\n",
      "608.1980240739014\n"
     ]
    }
   ],
   "source": [
    "print('Vanilla RNN - glove embedding Model text')\n",
    "print(gen_text_rnn_glove)\n",
    "print(ppl_score(gen_text_rnn_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM - glove embedding Model text\n",
      "Full of hope and passion, free more thrall of new, some some some at memory, them fell on some barbarian, see as it were bound.\n",
      "552.7443675864874\n"
     ]
    }
   ],
   "source": [
    "print('LSTM - glove embedding Model text')\n",
    "print(gen_text_lstm_glove)\n",
    "print(ppl_score(gen_text_lstm_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidirectional LSTM - glove embedding Model text\n",
      "Full of hope and passion, mind and say dryden's less, to me are good spread, me to say their eyes, awake to say their offends.\n",
      "445.25010160175884\n"
     ]
    }
   ],
   "source": [
    "print('Bidirectional LSTM - glove embedding Model text')\n",
    "print(gen_text_bilstm_glove)\n",
    "print(ppl_score(gen_text_bilstm_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
